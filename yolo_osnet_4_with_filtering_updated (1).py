import cv2
import torch
import json
import os
import sys
import numpy as np
from ultralytics import YOLO
from PIL import Image
from collections import defaultdict
from scipy.spatial.distance import cosine
from typing import List, Dict, Tuple, Set
import itertools
import copy
import matplotlib.pyplot as plt
from torchvision import transforms

# ============================================================================
# TorchReID
# ============================================================================
try:
    from torchreid import models, utils
    TORCHREID_AVAILABLE = True
except ImportError:
    print("âš ï¸ Warning: torchreid library not found. Re-ID will be disabled.")
    print("   Install with: pip install torchreid")
    TORCHREID_AVAILABLE = False

# ============================================================================
# DPT Depth Estimation
# ============================================================================
try:
    from transformers import DPTImageProcessor, DPTForDepthEstimation
    DPT_AVAILABLE = True
except ImportError:
    print("âš ï¸ Warning: transformers library not found. Depth estimation will be disabled.")
    print("   Install with: pip install transformers")
    DPT_AVAILABLE = False

# ============================================================================
# PART 0-A: í•„í„° ì„¤ì • ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================
def process_filter_config(filter_mode, filter_classes, model):
    """
    filter_modeì™€ filter_classesë¥¼ included_classes, excluded_classesë¡œ ë³€í™˜
    
    Args:
        filter_mode (int): 1=ë¸”ë™ë¦¬ìŠ¤íŠ¸, 2=í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, 3=í•„í„°ë§ì—†ìŒ
        filter_classes (list): í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì í˜¼ìš© ê°€ëŠ¥)
        model: YOLO/RT-DETR ëª¨ë¸ ê°ì²´
        
    Returns:
        tuple: (included_classes, excluded_classes)
            - filter_mode=1: (None, excluded_classes)
            - filter_mode=2: (included_classes, None)
            - filter_mode=3: (None, None)
    
    Raises:
        ValueError: filter_modeê°€ 1, 2, 3ì´ ì•„ë‹Œ ê²½ìš°
    """
    # 1. filter_mode ê²€ì¦
    if filter_mode not in [1, 2, 3]:
        raise ValueError(
            f"âŒ ì˜¤ë¥˜: filter_modeëŠ” 1, 2, 3 ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n"
            f"   - 1: ë¸”ë™ë¦¬ìŠ¤íŠ¸ (íŠ¹ì • í´ë˜ìŠ¤ ì œì™¸)\n"
            f"   - 2: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (íŠ¹ì • í´ë˜ìŠ¤ë§Œ í¬í•¨)\n"
            f"   - 3: í•„í„°ë§ ì—†ìŒ (ëª¨ë“  í´ë˜ìŠ¤ í¬í•¨)\n"
            f"   í˜„ì¬ ì…ë ¥ê°’: {filter_mode}"
        )
    
    # 2. filter_mode=3 (í•„í„°ë§ ì—†ìŒ)ì¸ ê²½ìš°
    if filter_mode == 3:
        if filter_classes:
            print(f"âš ï¸ ê²½ê³ : filter_mode=3 (í•„í„°ë§ ì—†ìŒ)ì´ë¯€ë¡œ filter_classes={filter_classes}ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.")
        print("âœ“ í•„í„°ë§ ëª¨ë“œ: ëª¨ë“  í´ë˜ìŠ¤ í¬í•¨ (í•„í„°ë§ ì—†ìŒ)")
        return None, None
    
    # 3. filter_mode=1 ë˜ëŠ” 2ì¸ë° filter_classesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
    if not filter_classes:
        mode_name = "ë¸”ë™ë¦¬ìŠ¤íŠ¸" if filter_mode == 1 else "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸"
        print(f"âš ï¸ ê²½ê³ : filter_mode={filter_mode} ({mode_name})ì¸ë° filter_classesê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        print("   â†’ í•„í„°ë§ ì—†ìŒìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
        return None, None
    
    # 4. í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ ìƒì„± (ìˆ«ì â†’ ì´ë¦„)
    class_names = model.names  # dict: {0: 'person', 1: 'bicycle', ...}
    
    # 5. filter_classesì˜ ìˆ«ìë¥¼ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    processed_classes = []
    for item in filter_classes:
        if isinstance(item, int):
            # ìˆ«ìì¸ ê²½ìš° í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
            if item in class_names:
                class_name = class_names[item]
                processed_classes.append(class_name)
                print(f"   - ìˆ«ì {item} â†’ '{class_name}' ë³€í™˜")
            else:
                print(f"âš ï¸ ê²½ê³ : í´ë˜ìŠ¤ ID {item}ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬´ì‹œí•©ë‹ˆë‹¤.")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ID: 0 ~ {len(class_names)-1}")
        elif isinstance(item, str):
            # ë¬¸ìì—´ì¸ ê²½ìš° ê²€ì¦ í›„ ì¶”ê°€
            if item in class_names.values():
                processed_classes.append(item)
            else:
                print(f"âš ï¸ ê²½ê³ : í´ë˜ìŠ¤ ì´ë¦„ '{item}'ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬´ì‹œí•©ë‹ˆë‹¤.")
                print(f"   --list-classes ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            print(f"âš ï¸ ê²½ê³ : '{item}'ëŠ” ë¬¸ìì—´ ë˜ëŠ” ì •ìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤. ë¬´ì‹œí•©ë‹ˆë‹¤.")
    
    # 6. ì¤‘ë³µ ì œê±°
    processed_classes = list(set(processed_classes))
    
    # 7. ìµœì¢… ê²€ì¦
    if not processed_classes:
        mode_name = "ë¸”ë™ë¦¬ìŠ¤íŠ¸" if filter_mode == 1 else "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸"
        print(f"âš ï¸ ê²½ê³ : ìœ íš¨í•œ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ({mode_name} ëª¨ë“œ)")
        print("   â†’ í•„í„°ë§ ì—†ìŒìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
        return None, None
    
    # 8. filter_modeì— ë”°ë¼ ë¶„ê¸°
    if filter_mode == 1:
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸: excluded_classes ì„¤ì •
        print(f"âœ“ í•„í„°ë§ ëª¨ë“œ: ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ë‹¤ìŒ í´ë˜ìŠ¤ ì œì™¸)")
        print(f"   ì œì™¸ í´ë˜ìŠ¤: {processed_classes}")
        return None, processed_classes
    elif filter_mode == 2:
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: included_classes ì„¤ì •
        print(f"âœ“ í•„í„°ë§ ëª¨ë“œ: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ë‹¤ìŒ í´ë˜ìŠ¤ë§Œ í¬í•¨)")
        print(f"   í¬í•¨ í´ë˜ìŠ¤: {processed_classes}")
        return processed_classes, None


# ============================================================================
# PART 0-B: DPT ëª¨ë¸ ë¡œë”© ë° Depth ì¶”ì • í•¨ìˆ˜
# ============================================================================
def load_dpt_model(model_name="Intel/dpt-large"):
    """
    DPT ëª¨ë¸ ë¡œë“œ
    
    Args:
        model_name: ì‚¬ìš©í•  DPT ëª¨ë¸
            - "Intel/dpt-large": ê³ í•´ìƒë„, ëŠë¦¼
            - "Intel/dpt-hybrid-midas": ì¤‘ê°„ ì„±ëŠ¥
            - "Intel/dpt-swinv2-tiny-256": ë¹ ë¦„, ì €í•´ìƒë„
    
    Returns:
        processor, model, device
    """
    if not DPT_AVAILABLE:
        print("âš ï¸ DPT model cannot be loaded. Skipping depth estimation.")
        return None, None, None
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ“Š Loading DPT model: {model_name}")
    
    try:
        processor = DPTImageProcessor.from_pretrained(model_name)
        model = DPTForDepthEstimation.from_pretrained(model_name)
        model = model.to(device)
        model.eval()
        print(f"âœ“ DPT model loaded successfully on {device}")
        return processor, model, device
    except Exception as e:
        print(f"âœ— Error loading DPT model: {e}")
        return None, None, None


def estimate_depth(frame, processor, model, device):
    """
    í”„ë ˆì„ì˜ depth map ì¶”ì •
    
    Args:
        frame: BGR ì´ë¯¸ì§€ (OpenCV format)
        processor: DPT processor
        model: DPT model
        device: cuda or cpu
    
    Returns:
        depth_map: normalized depth map (0-255)
        depth_array: raw depth values (float)
    """
    if processor is None or model is None:
        return None, None
    
    try:
        # BGR to RGB
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image_pil = Image.fromarray(image_rgb)
        
        # Preprocess
        inputs = processor(images=image_pil, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Inference
        with torch.no_grad():
            outputs = model(**inputs)
            predicted_depth = outputs.predicted_depth
        
        # Post-process
        prediction = torch.nn.functional.interpolate(
            predicted_depth.unsqueeze(1),
            size=frame.shape[:2],
            mode="bicubic",
            align_corners=False,
        )
        
        # Convert to numpy
        depth_array = prediction.squeeze().cpu().numpy()
        
        # Normalize to 0-255 for visualization
        depth_normalized = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min())
        depth_map = (depth_normalized * 255).astype(np.uint8)
        
        return depth_map, depth_array
        
    except Exception as e:
        print(f"âš ï¸ Error in depth estimation: {e}")
        return None, None


def extract_object_depth_info(depth_array, box, percentiles=[25, 50, 75], 
                             use_foreground_only=True, threshold_method='otsu',
                             save_mask=False, mask_save_path=None):
    """
    ê°ì²´ ì˜ì—­ì˜ depth í†µê³„ ì¶”ì¶œ (íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì „ê²½/ë°°ê²½ ë¶„ë¦¬)
    
    Args:
        depth_array: raw depth values
        box: [x1, y1, x2, y2]
        percentiles: ì¶”ì¶œí•  ë°±ë¶„ìœ„ìˆ˜
        use_foreground_only: Trueë©´ ì „ê²½(ê°ì²´)ë§Œ ì‚¬ìš©, Falseë©´ ì „ì²´ ì‚¬ìš©
        threshold_method: 'otsu' (Otsu's method) or 'mean' (í‰ê· ê°’ ê¸°ì¤€)
        save_mask: ë§ˆìŠ¤í¬ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€
        mask_save_path: ë§ˆìŠ¤í¬ ì €ì¥ ê²½ë¡œ
    
    Returns:
        depth_info: dict with depth statistics (ì „ê²½ depth ì •ë³´ í¬í•¨)
    """
    if depth_array is None:
        return None
    
    try:
        x1, y1, x2, y2 = map(int, box)
        
        # Boundary check
        h, w = depth_array.shape
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        # Extract object region
        object_depth = depth_array[y1:y2, x1:x2]
        
        if object_depth.size == 0:
            return None
        
        # Normalize to 0-255 for histogram/thresholding
        object_depth_norm = ((object_depth - object_depth.min()) / 
                            (object_depth.max() - object_depth.min() + 1e-8) * 255).astype(np.uint8)
        
        # Calculate threshold
        if threshold_method == 'otsu':
            # Otsu's methodë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            threshold, _ = cv2.threshold(object_depth_norm, 0, 255, 
                                        cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        elif threshold_method == 'mean':
            # í‰ê· ê°’ì„ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
            threshold = np.mean(object_depth_norm)
        else:
            threshold = 128  # default
        
        # Create foreground mask (depthê°€ thresholdë³´ë‹¤ í° ê°’ = ë” ê°€ê¹Œìš´ ê°ì²´)
        # DepthëŠ” ë³´í†µ ê°€ê¹Œìš¸ìˆ˜ë¡ í° ê°’ì´ë¯€ë¡œ, thresholdë³´ë‹¤ í° ê°’ì„ ì „ê²½ìœ¼ë¡œ ê°„ì£¼
        foreground_mask = object_depth_norm >= threshold
        
        # ì „ê²½ í”½ì…€ ìˆ˜ í™•ì¸
        foreground_pixel_count = np.sum(foreground_mask)
        total_pixel_count = object_depth.size
        foreground_ratio = foreground_pixel_count / total_pixel_count if total_pixel_count > 0 else 0
        
        # ë§ˆìŠ¤í¬ ì‹œê°í™” ì €ì¥ (ì˜µì…˜)
        if save_mask and mask_save_path:
            mask_vis = np.zeros_like(object_depth_norm)
            mask_vis[foreground_mask] = 255
            cv2.imwrite(mask_save_path, mask_vis)
        
        # ì „ê²½ë§Œ ì‚¬ìš©í• ì§€ ì „ì²´ë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •
        if use_foreground_only and foreground_pixel_count > 0:
            depth_values = object_depth[foreground_mask]
            depth_type = "foreground_only"
        else:
            depth_values = object_depth.flatten()
            depth_type = "all_pixels"
        
        if depth_values.size == 0:
            return None
        
        # Calculate statistics
        depth_info = {
            "segmentation_method": threshold_method,
            "threshold_value": float(threshold),
            "depth_type": depth_type,
            "foreground_pixel_count": int(foreground_pixel_count),
            "total_pixel_count": int(total_pixel_count),
            "foreground_ratio": float(foreground_ratio),
            "mean": float(np.mean(depth_values)),
            "median": float(np.median(depth_values)),
            "std": float(np.std(depth_values)),
            "min": float(np.min(depth_values)),
            "max": float(np.max(depth_values)),
            "percentiles": {
                f"p{p}": float(np.percentile(depth_values, p))
                for p in percentiles
            }
        }
        
        # ë°°ê²½ í†µê³„ë„ ì¶”ê°€ (ë¹„êµìš©)
        if use_foreground_only and foreground_pixel_count > 0:
            background_mask = ~foreground_mask
            background_pixel_count = np.sum(background_mask)
            if background_pixel_count > 0:
                background_values = object_depth[background_mask]
                depth_info["background_stats"] = {
                    "mean": float(np.mean(background_values)),
                    "median": float(np.median(background_values)),
                    "pixel_count": int(background_pixel_count)
                }
        
        return depth_info
        
    except Exception as e:
        print(f"âš ï¸ Error extracting depth info: {e}")
        return None


def visualize_depth_map(depth_map, output_path):
    """
    Depth mapì„ ì»¬ëŸ¬ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ì—¬ ì €ì¥
    
    Args:
        depth_map: normalized depth map (0-255)
        output_path: ì €ì¥ ê²½ë¡œ
    """
    if depth_map is None:
        return
    
    try:
        # Apply colormap (closer = warmer colors)
        depth_colored = cv2.applyColorMap(depth_map, cv2.COLORMAP_INFERNO)
        cv2.imwrite(output_path, depth_colored)
    except Exception as e:
        print(f"âš ï¸ Error saving depth visualization: {e}")


# ============================================================================
# PART 0.5: ê°ì²´ í•„í„°ë§ í•¨ìˆ˜ (NEW)
# ============================================================================
def should_include_detection(det, included_classes=None, excluded_classes=None):
    """
    detectionì´ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
    
    Args:
        det: detection ë”•ì…”ë„ˆë¦¬ (class_name í¬í•¨)
        included_classes: í¬í•¨í•  í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (None = ëª¨ë‘ í¬í•¨)
        excluded_classes: ì œì™¸í•  í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (None = ì œì™¸ ì—†ìŒ)
    
    Returns:
        bool: Trueë©´ í¬í•¨, Falseë©´ ì œì™¸
    """
    class_name = det.get('class_name', '')
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ì„  (ì§€ì •ëœ ê²ƒë§Œ í¬í•¨)
    if included_classes is not None:
        return class_name in included_classes
    
    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ì§€ì •ëœ ê²ƒ ì œì™¸)
    if excluded_classes is not None:
        return class_name not in excluded_classes
    
    # ë‘˜ ë‹¤ Noneì´ë©´ ëª¨ë‘ í¬í•¨
    return True


def print_available_classes(detection_model):
    """
    íƒì§€ ëª¨ë¸ì´ ì¸ì‹ ê°€ëŠ¥í•œ ëª¨ë“  í´ë˜ìŠ¤ ì¶œë ¥
    """
    class_names = detection_model.names
    print("\n" + "="*80)
    print("ğŸ“‹ AVAILABLE CLASSES FOR DETECTION")
    print("="*80)
    print(f"Total classes: {len(class_names)}\n")
    
    # í´ë˜ìŠ¤ë¥¼ 4ì—´ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
    classes_list = [f"{idx}: {name}" for idx, name in class_names.items()]
    
    for i in range(0, len(classes_list), 4):
        row = classes_list[i:i+4]
        print("  ".join(f"{item:<20}" for item in row))
    
    print("="*80)
    print("ğŸ’¡ TIP: Copy class names from above to use in 'excluded_classes' config")
    print("="*80 + "\n")
    
    return class_names


def list_classes_only(model_path, model_type="yolo"):
    """
    ëª¨ë¸ì˜ í´ë˜ìŠ¤ ëª©ë¡ë§Œ ì¶œë ¥í•˜ëŠ” ë…ë¦½ í•¨ìˆ˜ (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì—†ì´)
    
    Usage:
        python yolo_osnet_4_with_filtering.py --list-classes
    """
    print(f"\nğŸ” Loading {model_type.upper()} model: {model_path}")
    detection_model = YOLO(model_path)
    
    print_available_classes(detection_model)
    
    print("\nâœ… Use these class names in your config's 'excluded_classes' or 'included_classes'")
    print("Example:")
    print('  "excluded_classes": ["person", "car", "bicycle"]')
    print('  "included_classes": ["cell phone", "laptop", "bottle"]\n')


# ============================================================================
# PART 1 & 2: ëª¨ë¸ ë¡œë”©, í”„ë ˆì„ ì„ íƒ, Re-ID (TorchReID ê¸°ë°˜, ORB ì œê±°)
# ============================================================================
def load_models(model_type="yolo", model_path=None, torchreid_model_path=None, use_depth=True, dpt_model_name="Intel/dpt-large"):
    """Load detection model (YOLO or RT-DETR), TorchReID, and optionally DPT models"""
    if model_path is None:
        raise ValueError("model_path must be specified for the detection model.")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Load detection model (YOLO or RT-DETR via Ultralytics)
    print(f"\nğŸ“Š Loading detection model: {model_type.upper()} from {model_path}")
    if model_type.lower() == "yolo":
        detection_model = YOLO(model_path)
    elif model_type.lower() == "rt_detr":
        detection_model = YOLO(model_path)  # Ultralytics YOLO supports RT-DETR
    else:
        raise ValueError(f"Unsupported model_type: {model_type}. Use 'yolo' or 'rt_detr'.")
    
    # Load TorchReID model (OSNet)
    if not TORCHREID_AVAILABLE:
        print("âš ï¸ TorchReID model cannot be loaded. Skipping Re-ID.")
        torchreid_model = None
    else:
        print(f"\nğŸ“Š Loading TorchReID OSNet model")
        try:
            if torchreid_model_path is not None and os.path.exists(torchreid_model_path):
                # Load from local Market-1501 fine-tuned path (num_classes=751 for compatibility)
                checkpoint = torch.load(torchreid_model_path, map_location=device)
                torchreid_model = models.build_model(name='osnet_x1_0', num_classes=751)
                torchreid_model = torchreid_model.to(device)
                torchreid_model.load_state_dict(checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint)
                # Set classifier to None to extract features before classification (512-dim features)
                torchreid_model.classifier = None
                torchreid_model.eval()
                print(f"âœ“ TorchReID OSNet model loaded from local path: {torchreid_model_path} on {device}")
            else:
                # Load pretrained weights (ImageNet, no classifier needed)
                torchreid_model = models.build_model(name='osnet_x1_0', num_classes=0)
                torchreid_model = torchreid_model.to(device)
                utils.load_pretrained_weights(torchreid_model, 'osnet_x1_0')
                torchreid_model.eval()
                print(f"âœ“ TorchReID OSNet pretrained model loaded on {device}")
        except Exception as e:
            print(f"âœ— Error loading TorchReID model: {e}")
            torchreid_model = None
    
    class_names = detection_model.names
    
    # Load DPT if requested
    if use_depth:
        dpt_processor, dpt_model, dpt_device = load_dpt_model(dpt_model_name)
    else:
        dpt_processor, dpt_model, dpt_device = None, None, None
    
    return detection_model, torchreid_model, device, class_names, dpt_processor, dpt_model, dpt_device

def primary_selection(video_path, detection_model, class_names, frame_skip_interval=1,
                     included_classes=None, excluded_classes=None, tracker="botsort.yaml"):
    """Primary selection: group frames by object set changes (with frame skipping and class filtering)"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened(): raise ValueError("Error opening video file")

    frame_idx, prev_track_set, groups, current_group = 0, set(), [], []
    skip_counter = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        # Frame skipping logic
        skip_counter += 1
        if skip_counter < frame_skip_interval:
            frame_idx += 1
            continue
        skip_counter = 0

        results = detection_model.track(frame, persist=True, verbose=False, tracker=tracker)
        current_track_set = set(results[0].boxes.id.cpu().numpy()) if results[0].boxes.id is not None else set()
        if current_track_set != prev_track_set:
            if current_group: groups.append(current_group)
            current_group = []
        if results[0].boxes.id is not None:
            # í•„í„°ë§ ì ìš©
            detections = []
            for b, c, l, t in zip(results[0].boxes.xyxy.cpu(), results[0].boxes.conf.cpu(), 
                                 results[0].boxes.cls.cpu(), results[0].boxes.id.cpu()):
                det = {
                    'box': b.tolist(), 
                    'conf': float(c), 
                    'cls': int(l), 
                    'class_name': class_names[int(l)], 
                    'track_id': int(t)
                }
                # í•„í„°ë§ ì²´í¬
                if should_include_detection(det, included_classes, excluded_classes):
                    detections.append(det)
            
            if detections:  # í•„í„°ë§ í›„ ë‚¨ì€ detectionì´ ìˆì„ ë•Œë§Œ ì¶”ê°€
                current_group.append((frame_idx, frame, detections))
        prev_track_set = current_track_set
        frame_idx += 1
    if current_group: groups.append(current_group)
    cap.release()
    return [group[len(group) // 2] for group in groups if group]

def calculate_combined_histogram(frame, weight_brightness=0.5, weight_saturation=0.5):
    """
    [NEW] ê²°í•© íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°: Grayscale brightness + HSV S saturation (50:50 ê¸°ë³¸)
    
    Args:
        frame: BGR ì´ë¯¸ì§€
        weight_brightness: ë°ê¸° ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.5)
        weight_saturation: ì±„ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.5)
    
    Returns:
        combined_hist: ì •ê·œí™”ëœ ê²°í•© íˆìŠ¤í† ê·¸ë¨ (flatten)
    """
    # Brightness (Grayscale)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    brightness_hist = cv2.normalize(cv2.calcHist([gray], [0], None, [256], [0, 256]), None).flatten()
    
    # Saturation (HSV S ì±„ë„)
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    s_channel = hsv[:, :, 1]
    saturation_hist = cv2.normalize(cv2.calcHist([s_channel], [0], None, [256], [0, 256]), None).flatten()
    
    # ê²°í•©
    combined_hist = weight_brightness * brightness_hist + weight_saturation * saturation_hist
    return combined_hist

def bhattacharyya_distance(hist1, hist2):
    bc = np.sum(np.sqrt(hist1 * hist2))
    return -np.log(bc) if bc > 0 else float('inf')

def profile_tracking(selected_frames, window_size=5, hist_threshold=0.3, 
                     weight_brightness=0.5, weight_saturation=0.5, iterations=1):
    """
    [MODIFIED] Profile Tracking: ê²°í•© íˆìŠ¤í† ê·¸ë¨ ì‚¬ìš© + ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„° ì¶”ê°€ + ë°˜ë³µ iteration ì§€ì› + Bidirectional Window Size ì‹¤ì œ ì ìš©
    """
    if not selected_frames: return []
    print(f"Using combined histogram: Brightness {weight_brightness*100:.0f}% + Saturation {weight_saturation*100:.0f}%")
    print(f"Profile Tracking iterations: {iterations}, window_size: {window_size} (Â±{window_size//2} frames, bidirectional)")
    
    current_frames = selected_frames
    for iter_num in range(iterations):
        print(f"  Iteration {iter_num + 1}/{iterations}")
        frame_histograms = [(f_idx, f, d, calculate_combined_histogram(f, weight_brightness, weight_saturation), 
                             set(det['track_id'] for det in d)) for f_idx, f, d in current_frames]
        keep_flags = [True] * len(frame_histograms)
        half_window = window_size // 2
        for i in range(len(frame_histograms)):
            if not keep_flags[i]: continue
            _, _, _, hist_i, track_ids_i = frame_histograms[i]
            # Bidirectional Window ì ìš©: i - half_window ~ i + half_window (ìì‹  ì œì™¸)
            start_j = max(0, i - half_window)
            end_j = min(len(frame_histograms), i + half_window + 1)
            for j in range(start_j, end_j):
                if j == i or not keep_flags[j]: continue  # ìì‹  & ì´ë¯¸ ì œê±°ëœ ìŠ¤í‚µ
                _, _, _, hist_j, track_ids_j = frame_histograms[j]
                if bhattacharyya_distance(hist_i, hist_j) < hist_threshold:
                    if track_ids_i.issubset(track_ids_j):
                        keep_flags[i] = False
                        break
                    elif track_ids_j.issubset(track_ids_i):
                        keep_flags[j] = False
        current_frames = [current_frames[i] for i, keep in enumerate(keep_flags) if keep]
        print(f"    -> Remaining frames after iteration {iter_num + 1}: {len(current_frames)}")
    
    return current_frames

def extract_torchreid_features(crop_bgr, torchreid_model, device):
    """
    TorchReID ëª¨ë¸ë¡œ ê°ì²´ í¬ë¡­ì˜ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
    
    Args:
        crop_bgr: BGR í˜•ì‹ ê°ì²´ í¬ë¡­ ì´ë¯¸ì§€
        torchreid_model: TorchReID ëª¨ë¸
        device: cuda or cpu
    
    Returns:
        features: íŠ¹ì§• ë²¡í„° (numpy array)
    """
    if torchreid_model is None:
        return None
    
    try:
        # BGR to RGB
        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
        crop_pil = Image.fromarray(crop_rgb)
        
        # TorchReID transforms (standard for ReID)
        transform = transforms.Compose([
            transforms.Resize((256, 128)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # Preprocess
        tensor = transform(crop_pil).unsqueeze(0).to(device)
        
        # Inference (now returns 512-dim features since classifier is None)
        with torch.no_grad():
            features = torchreid_model(tensor)
        
        return features.squeeze().cpu().numpy()
    except Exception as e:
        print(f"âš ï¸ Error extracting TorchReID features: {e}")
        return None

def unified_torchreid_reid_and_frame_selection(filtered_frames, torchreid_model, device, 
                                               reid_threshold=0.8, 
                                               frame_merge_threshold=0.9, min_samples=2,
                                               included_classes=None, excluded_classes=None):
    """
    [MODIFIED] TorchReID ê¸°ë°˜ Re-ID ë° í”„ë ˆì„ ì„ íƒ (ORB ì œê±°, ë‹¨ì¼ threshold ì‚¬ìš©, í•„í„°ë§ ì¶”ê°€)
    """
    if torchreid_model is None:
        print("âš ï¸ TorchReID not available. Skipping Re-ID and returning filtered frames.")
        return [(f_idx, f, d) for f_idx, f, d in filtered_frames]
    
    all_frame_data = []
    for frame_idx, frame, detections in filtered_frames:
        frame_objects = []
        for det in detections:
            # í•„í„°ë§ ì²´í¬
            if not should_include_detection(det, included_classes, excluded_classes):
                continue
            
            box = det['box']
            crop_bgr = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]
            reid_feature = extract_torchreid_features(crop_bgr, torchreid_model, device)
            if reid_feature is None:
                continue  # Skip if feature extraction fails
            frame_objects.append({'class_name': det['class_name'], 'track_id': det['track_id'], 'original_track_id': det['track_id'], 'reid_feature': reid_feature, 'crop_bgr': crop_bgr, 'conf': det['conf'], 'box': det['box'], 'cls': det['cls']})
        
        if frame_objects:  # í•„í„°ë§ í›„ ë‚¨ì€ ê°ì²´ê°€ ìˆì„ ë•Œë§Œ ì¶”ê°€
            all_frame_data.append({'frame_idx': frame_idx, 'frame': frame, 'objects': frame_objects})
    
    objects_by_class = defaultdict(list)
    for frame_data in all_frame_data:
        for obj in frame_data['objects']: objects_by_class[obj['class_name']].append(obj)
    
    global_id_mapping = {}
    for class_name, objects in objects_by_class.items():
        if len(objects) < min_samples: continue
        clusters, assigned = [], set()
        for i, obj_i in enumerate(objects):
            if i in assigned: continue
            cluster, assigned_in_cluster = [i], {i}
            for j, obj_j in enumerate(objects):
                if j in assigned or j in assigned_in_cluster: continue
                reid_sim = 1 - cosine(obj_i['reid_feature'], obj_j['reid_feature'])
                if reid_sim > reid_threshold:
                    cluster.append(j); assigned_in_cluster.add(j)
            assigned.update(assigned_in_cluster)
            clusters.append(cluster)
        for cluster_id, cluster in enumerate(clusters, 1):
            for obj_idx in cluster:
                key = (objects[obj_idx]['class_name'], objects[obj_idx]['track_id'])
                if key not in global_id_mapping: global_id_mapping[key] = f"{class_name}_{cluster_id}"
    
    for frame_data in all_frame_data:
        for obj in frame_data['objects']:
            key = (obj['class_name'], obj['original_track_id'])
            obj['track_id'] = global_id_mapping.get(key, f"unclustered_{obj['original_track_id']}")
    
    unique_frames, merged = [], set()
    for i, frame_data_i in enumerate(all_frame_data):
        if i in merged: continue
        objects_i = {(obj['class_name'], obj['track_id']): obj['reid_feature'] for obj in frame_data_i['objects']}
        for j in range(i + 1, len(all_frame_data)):
            if j in merged: continue
            objects_j = {(obj['class_name'], obj['track_id']): obj['reid_feature'] for obj in all_frame_data[j]['objects']}
            common_keys = set(objects_i.keys()) & set(objects_j.keys())
            if not common_keys or len(objects_i) != len(objects_j): continue
            if sum(1 for key in common_keys if 1 - cosine(objects_i[key], objects_j[key]) > frame_merge_threshold) / len(objects_i) > 0.95:
                merged.add(j)
        detections = [{'box': o['box'], 'conf': o['conf'], 'cls': o['cls'], 'class_name': o['class_name'], 'track_id': o['track_id'], 'original_track_id': o['original_track_id']} for o in frame_data_i['objects']]
        unique_frames.append((frame_data_i['frame_idx'], frame_data_i['frame'], detections))
    return unique_frames

# ============================================================================
# PART 3: Greedy Selection
# ============================================================================
def extract_object_key(det: Dict, use_instance_id: bool = False) -> str:
    if use_instance_id and ("track_id" in det):
        return str(det['track_id'])
    else:
        return str(det.get('class_name'))

def image_object_set(detections: List[Dict], use_instance_id: bool = False, score_thresh: float = 0.0,
                    included_classes=None, excluded_classes=None) -> Set[str]:
    return {
        extract_object_key(d, use_instance_id) 
        for d in detections 
        if d.get("conf", 1.0) >= score_thresh
        and should_include_detection(d, included_classes, excluded_classes)
    }

def combos_from_set(objset: Set[str], k: int) -> Set[Tuple[str, ...]]:
    if len(objset) < k:
        return set()
    return {tuple(sorted(c)) for c in itertools.combinations(objset, k)}

def greedy_coverage_selection(
    frames_data: List[Tuple], k: int = 2, use_instance_id: bool = False, 
    max_selected: int = None, min_new_combos: int = 1, score_thresh: float = 0.0,
    included_classes=None, excluded_classes=None
) -> (List[Dict], Dict): 
    n = len(frames_data)
    frame_objsets = [image_object_set(d, use_instance_id, score_thresh, included_classes, excluded_classes) 
                     for _, _, d in frames_data]
    frame_combos = [combos_from_set(o, k) for o in frame_objsets]
    greedy_log = {
        "parameters": {
            "k": k, "use_instance_id": use_instance_id, "max_selected": max_selected,
            "min_new_combos": min_new_combos, "score_thresh": score_thresh
        },
        "initial_state": {},
        "iterations": [],
        "final_result": {}
    }
    total_unique_combos = set.union(*frame_combos) if frame_combos else set()
    greedy_log["initial_state"]["total_unique_combos"] = sorted([list(c) for c in total_unique_combos])
    greedy_log["initial_state"]["total_unique_count"] = len(total_unique_combos)
    
    selected = []
    covered_combos: Set[Tuple[str, ...]] = set()
    remaining = set(range(n))
    iteration_count = 0
    
    print(f"\n{'='*60}\nStarting Greedy Coverage Selection with Logging\n{'='*60}")
    
    while remaining:
        iteration_count += 1
        iteration_log = {
            "iteration": iteration_count,
            "covered_before_count": len(covered_combos),
            "covered_before": sorted([list(c) for c in covered_combos]),
            "candidates": []
        }
        best_idx = -1
        best_new_count = -1
        best_new_set = set()
        candidate_details = []
        for i in remaining:
            new_combos = frame_combos[i] - covered_combos
            new_count = len(new_combos)
            candidate_details.append({
                "candidate_index": i,
                "original_frame_idx": frames_data[i][0],
                "new_count": new_count,
                "new_combos_offered": sorted([list(c) for c in new_combos])
            })
            if new_count > best_new_count:
                best_new_count = new_count
                best_new_set = new_combos
                best_idx = i
        
        iteration_log["candidates"] = sorted(candidate_details, key=lambda x: x['new_count'], reverse=True)
        if best_idx == -1 or best_new_count < min_new_combos:
            iteration_log["selection"] = f"STOP: Best new count ({best_new_count}) is less than min_new_combos ({min_new_combos})."
            greedy_log["iterations"].append(iteration_log)
            break
        
        original_frame_idx = frames_data[best_idx][0]
        iteration_log["selection"] = {
            "selected_index": best_idx,
            "original_frame_idx": original_frame_idx,
            "new_count_added": best_new_count,
            "new_combos_added": sorted([list(c) for c in best_new_set])
        }
        
        frame_idx, frame, detections = frames_data[best_idx]
        selected.append({
            "frame_idx": frame_idx, "frame": frame, "detections": copy.deepcopy(detections),
            "new_combos_added": [list(c) for c in best_new_set],
            "new_count": best_new_count,
        })
        
        print(f"Iteration {iteration_count}: Selected Frame {original_frame_idx} | New Pairs: {best_new_count}")
        
        covered_combos.update(best_new_set)
        remaining.remove(best_idx)
        
        iteration_log["covered_after_count"] = len(covered_combos)
        iteration_log["covered_after"] = sorted([list(c) for c in covered_combos])
        greedy_log["iterations"].append(iteration_log)
        
        if max_selected is not None and len(selected) >= max_selected:
            iteration_log["selection"]["stop_reason"] = f"Reached max_selected limit of {max_selected}."
            break
            
    final_coverage = len(covered_combos)
    total_coverage = len(total_unique_combos)
    coverage_percentage = (final_coverage / total_coverage * 100) if total_coverage > 0 else 0

    greedy_log["final_result"] = {
        "selected_frame_indices": [s['frame_idx'] for s in selected],
        "total_frames_selected": len(selected),
        "final_covered_combos_count": final_coverage,
        "total_unique_combos_count": total_coverage,
        "final_coverage_percentage": f"{coverage_percentage:.2f}%"
    }

    print(f"\nGreedy Selection Complete! Selected {len(selected)} frames.")
    print(f"Covered {final_coverage} out of {total_coverage} unique combinations ({coverage_percentage:.2f}%).\n{'='*60}\n")
    
    return selected, greedy_log

# ============================================================================
# PART 3.5: Post-Greedy Profile Tracking (ìƒˆ ì¶”ê°€)
# ============================================================================
def post_greedy_profile_tracking(final_selected, hist_threshold=0.25, 
                                 weight_brightness=0.5, weight_saturation=0.5, window_size=3, iterations=1):
    """
    [NEW] Greedy í›„ ì¶”ê°€ Profile Tracking: ìœ ì‚¬ í”„ë ˆì„ ì¬í•„í„°ë§ (iterations ë° window_size ì§€ì›)
    """
    if not final_selected: 
        return [], {"removed_count": 0, "removed_frames": []}
    
    print(f"Post-Greedy Filtering: Combined histogram (Brightness {weight_brightness*100:.0f}% + Saturation {weight_saturation*100:.0f}%), threshold={hist_threshold}, window_size={window_size}, iterations={iterations}")
    
    # final_selectedë¥¼ Tuple í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (frame_idx, frame, detections)
    converted_frames = [(item['frame_idx'], item['frame'], item['detections']) for item in final_selected]
    
    # profile_tracking ë¡œì§ ì¬ì‚¬ìš© (window_size ë° iterations ì „ë‹¬)
    filtered_frames = profile_tracking(converted_frames, window_size=window_size, 
                                       hist_threshold=hist_threshold,
                                       weight_brightness=weight_brightness, 
                                       weight_saturation=weight_saturation,
                                       iterations=iterations)
    
    # ì›ë³¸ final_selectedì™€ ë§¤ì¹­í•´ í•„í„°ë§ëœ ì¸ë±ìŠ¤ ì°¾ê¸° (frame_idxë¡œë§Œ ë¹„êµ)
    filtered_frame_indices = {frame_idx for frame_idx, _, _ in filtered_frames}
    ultra_filtered = [item for item in final_selected if item['frame_idx'] in filtered_frame_indices]
    
    removed_count = len(final_selected) - len(ultra_filtered)
    removed_frames = [item['frame_idx'] for item in final_selected if item['frame_idx'] not in filtered_frame_indices]
    
    reduction_log = {
        "removed_count": removed_count,
        "removed_frames": removed_frames,
        "reduction_ratio": removed_count / len(final_selected) if final_selected else 0
    }
    
    print(f"-> Removed {removed_count} frames (ratio: {reduction_log['reduction_ratio']:.2f}). Remaining: {len(ultra_filtered)}")
    return ultra_filtered, reduction_log

# ============================================================================
# PART 4: ì‹œê°í™” ë° ê¸°ë³¸ ì €ì¥ í•¨ìˆ˜
# ============================================================================

def draw_detections(frame, detections, use_original_id=False):
    """í”„ë ˆì„ì— íƒì§€ ê²°ê³¼ë¥¼ ê·¸ë¦¬ëŠ” ë²”ìš© í•¨ìˆ˜"""
    frame_copy = frame.copy()
    
    track_ids_in_frame = []
    if detections:
        key_to_use = 'original_track_id' if use_original_id and 'original_track_id' in detections[0] else 'track_id'
        track_ids_in_frame = [det[key_to_use] for det in detections if key_to_use in det]

    unique_ids = sorted(list(set(track_ids_in_frame)))
    
    if not unique_ids:
        colors = [plt.cm.get_cmap('hsv', 20)(i) for i in range(20)]
        color_map = {i: (int(c[0]*255), int(c[1]*255), int(c[2]*255)) for i,c in enumerate(colors)}
    else:
        colors = [plt.cm.get_cmap('hsv', len(unique_ids))(i) for i in range(len(unique_ids))]
        color_map = {uid: (int(c[0]*255), int(c[1]*255), int(c[2]*255)) for uid, c in zip(unique_ids, colors)}

    for i, det in enumerate(detections):
        box = det['box']
        id_key = 'original_track_id' if use_original_id and 'original_track_id' in det else 'track_id'
        
        label = str(det.get(id_key, f"Obj-{i}"))
        color_key = det.get(id_key, i % 20)
        
        color = color_map.get(color_key, (0, 0, 255))
        
        cv2.rectangle(frame_copy, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)
        cv2.putText(frame_copy, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
    return frame_copy

def save_frames_as_images(frames_data, output_folder, prefix="", use_original_id=False):
    """í‚¤í”„ë ˆì„ ëª©ë¡ì„ ê°œë³„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    print(f"\nSaving frames to '{output_folder}'...")
    for frame_idx, frame, detections in frames_data:
        drawn_frame = draw_detections(frame, detections, use_original_id)
        output_path = os.path.join(output_folder, f"{prefix}frame_{frame_idx}.jpg")
        cv2.imwrite(output_path, drawn_frame)
    print(f"âœ“ Saved {len(frames_data)} frames.")

def create_video_from_source(video_path, detection_model, class_names, output_path, use_tracking=True,
                            included_classes=None, excluded_classes=None, tracker="botsort.yaml"):
    """ì›ë³¸ ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì¶”ì  ì ìš©/ë¯¸ì ìš© ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (í•„í„°ë§ ì¶”ê°€)"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error opening video file")
        return

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    print(f"\nCreating video: '{os.path.basename(output_path)}' (Tracking: {use_tracking})...")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if use_tracking:
            results = detection_model.track(frame, persist=True, verbose=False, tracker=tracker)
            if results[0].boxes is not None and results[0].boxes.id is not None:
                detections = []
                for i, (b, t) in enumerate(zip(results[0].boxes.xyxy.cpu(), results[0].boxes.id.cpu())):
                    det = {
                        'box': b.tolist(), 
                        'track_id': int(t),
                        'class_name': class_names[int(results[0].boxes.cls[i])]
                    }
                    if should_include_detection(det, included_classes, excluded_classes):
                        detections.append(det)
            else:
                detections = []
        else:
            results = detection_model.predict(frame, verbose=False)
            if results[0].boxes is not None:
                detections = []
                for i, b in enumerate(results[0].boxes.xyxy.cpu()):
                    det = {
                        'box': b.tolist(),
                        'class_name': class_names[int(results[0].boxes.cls[i])]
                    }
                    if should_include_detection(det, included_classes, excluded_classes):
                        detections.append(det)
            else:
                detections = []
        
        drawn_frame = draw_detections(frame, detections)
        out.write(drawn_frame)
        
    cap.release()
    out.release()
    print(f"âœ“ Video saved successfully to: {output_path}")

# ============================================================================
# PART 4-1: Depth ì •ë³´ë¥¼ í¬í•¨í•œ JSON ì €ì¥ í•¨ìˆ˜ë“¤ (ê°„ê²° êµ¬ì¡°)
# ============================================================================

def save_json_unified(final_selected, greedy_log, output_folder, 
                      dpt_processor=None, dpt_model=None, dpt_device=None,
                      use_foreground_only=True, threshold_method='otsu',
                      histogram_method="brightness_saturation_combined",
                      hist_weights={"brightness": 0.5, "saturation": 0.5},
                      post_filter=None):
    """
    [MODIFIED] unified ëª¨ë“œ: í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ëª¨ë“  í•µì‹¬ ì •ë³´ ì €ì¥ (PLAN ê¸°ë°˜ ê°„ê²° êµ¬ì¡°)
    """
    output_path = os.path.join(output_folder, "keyframe_summary_unified.json")
    
    print(f"\n{'='*60}\nSaving unified JSON (core info only)...\n{'='*60}")
    
    # Depth ì¶”ì • ë° ë°ì´í„° ì¶”ì¶œ
    frames_data = []
    depth_enabled = dpt_model is not None
    for idx, frame_data in enumerate(final_selected):
        frame_idx = frame_data['frame_idx']
        frame = frame_data['frame']
        
        # Depth ì¶”ì • (í•„ìš” ì‹œ)
        depth_map, depth_array = None, None
        if depth_enabled:
            depth_map, depth_array = estimate_depth(frame, dpt_processor, dpt_model, dpt_device)
        
        # í”„ë ˆì„ ë°ì´í„° êµ¬ì„± (ê°„ê²°í™”)
        frame_info = {
            "frame_index": frame_idx,
            "selection_order": idx + 1,
            "new_combinations_count": frame_data.get('new_count', 0),
            "detections": []
        }
        
        # Detection ê°„ê²°í™” + foreground depth (mean, stdë§Œ)
        for det in frame_data['detections']:
            det_info = {
                "class_name": det.get('class_name', 'unknown'),
                "track_id": det.get('track_id', 'unknown'),
                "bounding_box": {
                    "x1": det['box'][0],
                    "y1": det['box'][1],
                    "x2": det['box'][2],
                    "y2": det['box'][3]
                }
            }
            
            # Foreground depth ì¶”ê°€ (depth_enabled ì‹œ)
            if depth_enabled and depth_array is not None:
                object_depth_info = extract_object_depth_info(
                    depth_array, det['box'],
                    use_foreground_only=use_foreground_only,
                    threshold_method=threshold_method,
                    save_mask=False  # unified ëª¨ë“œë¼ ë§ˆìŠ¤í¬ ì €ì¥ ì•ˆ í•¨
                )
                if object_depth_info and object_depth_info['depth_type'] == 'foreground_only':
                    det_info["foreground_depth"] = {
                        "mean": object_depth_info['mean'],
                        "std": object_depth_info['std']
                    }
            
            frame_info['detections'].append(det_info)
        
        frames_data.append(frame_info)
    
    # ì „ì²´ ìš”ì•½ JSON êµ¬ì„±
    coverage_percentage = greedy_log['final_result'].get('final_coverage_percentage', '0%')
    unified_json = {
        "metadata": {
            "total_frames_selected": len(final_selected),
            "coverage_percentage": coverage_percentage,
            "depth_enabled": depth_enabled,
            "threshold_method": threshold_method if depth_enabled else None,
            "use_foreground_only": use_foreground_only if depth_enabled else None,
            "histogram_method": histogram_method,
            "hist_weights": hist_weights,
            "post_filter": post_filter or {"applied": False},
            "parameters": greedy_log.get('parameters', {})
        },
        "frames": frames_data,
        "explanations": {
            "frame_index": "ì›ë³¸ ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘, ì„ íƒëœ í‚¤í”„ë ˆì„ì˜ ìœ„ì¹˜).",
            "selection_order": "Greedy selectionì—ì„œ ì„ íƒëœ ìˆœì„œ (1ë¶€í„° ì‹œì‘, ë§ˆì§€ë§‰ ì„ íƒëœ í”„ë ˆì„ì¼ìˆ˜ë¡ ë†’ìŒ).",
            "new_combinations_count": "ì´ í”„ë ˆì„ì´ ì¶”ê°€ë¡œ ì»¤ë²„í•œ ê°ì²´ ìŒ(combination)ì˜ ê°œìˆ˜ (k=2 ê¸°ì¤€).",
            "class_name": "YOLOê°€ ê°ì§€í•œ ê°ì²´ í´ë˜ìŠ¤ (e.g., 'person', 'cell phone').",
            "track_id": "Re-ID í›„ì˜ ê³ ìœ  íŠ¸ë™ ID (í´ë˜ìŠ¤ëª…_ìˆ«ì í˜•ì‹, ë™ì¼ ê°ì²´ ì¶”ì ìš©).",
            "bounding_box.x1": "ë°•ìŠ¤ ì™¼ìª½ ìƒë‹¨ x ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
            "bounding_box.y1": "ë°•ìŠ¤ ì™¼ìª½ ìƒë‹¨ y ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
            "bounding_box.x2": "ë°•ìŠ¤ ì˜¤ë¥¸ìª½ í•˜ë‹¨ x ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
            "bounding_box.y2": "ë°•ìŠ¤ ì˜¤ë¥¸ìª½ í•˜ë‹¨ y ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
            "foreground_depth.mean": "ê°ì²´ ì „ê²½ ì˜ì—­ì˜ depth í‰ê· ê°’ (DPT ëª¨ë¸ ê¸°ì¤€, ê°€ê¹Œìš¸ìˆ˜ë¡ ê°’ì´ í¼).",
            "foreground_depth.std": "ê°ì²´ ì „ê²½ ì˜ì—­ì˜ depth í‘œì¤€í¸ì°¨ (depth ë¶„í¬ì˜ í¼ì§ ì •ë„).",
            "histogram_method": "í”„ë ˆì„ í•„í„°ë§ì— ì‚¬ìš©ëœ íˆìŠ¤í† ê·¸ë¨ ë°©ë²• (brightness + saturation ê²°í•©).",
            "post_filter": "Greedy í›„ ì¶”ê°€ ìœ ì‚¬ í”„ë ˆì„ í•„í„°ë§ ë‹¨ê³„."
        }
    }
    
    # ì €ì¥
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(unified_json, f, indent=2, ensure_ascii=False)
        print(f"âœ“ Successfully saved unified JSON to: {output_path}")
    except Exception as e:
        print(f"âœ— Error saving unified JSON: {e}")


def save_json_per_frame(final_selected, greedy_log, output_folder, 
                        dpt_processor=None, dpt_model=None, dpt_device=None,
                        save_depth_visualization=True, save_foreground_masks=True,
                        use_foreground_only=True, threshold_method='otsu',
                        histogram_method="brightness_saturation_combined",
                        hist_weights={"brightness": 0.5, "saturation": 0.5},
                        post_filter=None):
    """
    [MODIFIED] per_frame ëª¨ë“œ: ê° í”„ë ˆì„ë³„ë¡œ ê°œë³„ JSON ì €ì¥ + ê°„ê²° êµ¬ì¡° (PLAN ê¸°ë°˜)
    """
    # ê°œë³„ í”„ë ˆì„ í´ë” ìƒì„±
    per_frame_folder = os.path.join(output_folder, "per_frame_json")
    if not os.path.exists(per_frame_folder):
        os.makedirs(per_frame_folder)
    
    # Depth visualization í´ë” (ì˜µì…˜)
    if save_depth_visualization and dpt_model is not None:
        depth_vis_folder = os.path.join(output_folder, "depth_visualizations")
        if not os.path.exists(depth_vis_folder):
            os.makedirs(depth_vis_folder)
    
    # Foreground mask í´ë” (ì˜µì…˜)
    if save_foreground_masks and dpt_model is not None:
        mask_folder = os.path.join(output_folder, "foreground_masks")
        if not os.path.exists(mask_folder):
            os.makedirs(mask_folder)
    
    print(f"\n{'='*60}\nSaving per-frame JSON files (core info only)...\n{'='*60}")
    
    # Depth ì¶”ì • ë° ë°ì´í„° ì¶”ì¶œ (per_frameìš©)
    depth_enabled = dpt_model is not None
    explanations = {
        "frame_index": "ì›ë³¸ ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘, ì„ íƒëœ í‚¤í”„ë ˆì„ì˜ ìœ„ì¹˜).",
        "selection_order": "Greedy selectionì—ì„œ ì„ íƒëœ ìˆœì„œ (1ë¶€í„° ì‹œì‘, ë§ˆì§€ë§‰ ì„ íƒëœ í”„ë ˆì„ì¼ìˆ˜ë¡ ë†’ìŒ).",
        "new_combinations_count": "ì´ í”„ë ˆì„ì´ ì¶”ê°€ë¡œ ì»¤ë²„í•œ ê°ì²´ ìŒ(combination)ì˜ ê°œìˆ˜ (k=2 ê¸°ì¤€).",
        "class_name": "YOLOê°€ ê°ì§€í•œ ê°ì²´ í´ë˜ìŠ¤ (e.g., 'person', 'cell phone').",
        "track_id": "Re-ID í›„ì˜ ê³ ìœ  íŠ¸ë™ ID (í´ë˜ìŠ¤ëª…_ìˆ«ì í˜•ì‹, ë™ì¼ ê°ì²´ ì¶”ì ìš©).",
        "bounding_box.x1": "ë°•ìŠ¤ ì™¼ìª½ ìƒë‹¨ x ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
        "bounding_box.y1": "ë°•ìŠ¤ ì™¼ìª½ ìƒë‹¨ y ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
        "bounding_box.x2": "ë°•ìŠ¤ ì˜¤ë¥¸ìª½ í•˜ë‹¨ x ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
        "bounding_box.y2": "ë°•ìŠ¤ ì˜¤ë¥¸ìª½ í•˜ë‹¨ y ì¢Œí‘œ (í”½ì…€ ë‹¨ìœ„).",
        "foreground_depth.mean": "ê°ì²´ ì „ê²½ ì˜ì—­ì˜ depth í‰ê· ê°’ (DPT ëª¨ë¸ ê¸°ì¤€, ê°€ê¹Œìš¸ìˆ˜ë¡ ê°’ì´ í¼).",
        "foreground_depth.std": "ê°ì²´ ì „ê²½ ì˜ì—­ì˜ depth í‘œì¤€í¸ì°¨ (depth ë¶„í¬ì˜ í¼ì§ ì •ë„).",
        "histogram_method": "í”„ë ˆì„ í•„í„°ë§ì— ì‚¬ìš©ëœ íˆìŠ¤í† ê·¸ë¨ ë°©ë²• (brightness + saturation ê²°í•©).",
        "post_filter": "Greedy í›„ ì¶”ê°€ ìœ ì‚¬ í”„ë ˆì„ í•„í„°ë§ ë‹¨ê³„."
    }
    
    # ê° í”„ë ˆì„ë³„ JSON ì €ì¥
    for idx, frame_data in enumerate(final_selected):
        frame_idx = frame_data['frame_idx']
        frame = frame_data['frame']
        
        # Depth ì¶”ì •
        depth_map, depth_array = None, None
        if depth_enabled:
            print(f"  ğŸ“Š Estimating depth for frame {frame_idx}...")
            depth_map, depth_array = estimate_depth(frame, dpt_processor, dpt_model, dpt_device)
            
            # Depth visualization ì €ì¥
            if save_depth_visualization and depth_map is not None:
                depth_vis_path = os.path.join(depth_vis_folder, f"depth_frame_{frame_idx}.jpg")
                visualize_depth_map(depth_map, depth_vis_path)
        
        # í”„ë ˆì„ ë©”íƒ€ë°ì´í„° êµ¬ì„± (ê°„ê²°í™”)
        frame_metadata = {
            "frame_index": frame_idx,
            "selection_order": idx + 1,
            "new_combinations_count": frame_data.get('new_count', 0),
            "depth_available": depth_array is not None,
            "detections": []
        }
        
        # Detection ì •ë³´ ì¶”ê°€ + Depth ì •ë³´ (ê°„ê²°)
        for det_idx, det in enumerate(frame_data['detections']):
            # ê°ì²´ë³„ depth ì •ë³´ ì¶”ì¶œ (ì „ê²½/ë°°ê²½ ë¶„ë¦¬)
            object_depth_info = None
            if depth_array is not None:
                # ë§ˆìŠ¤í¬ ì €ì¥ ê²½ë¡œ ì„¤ì • (ì˜µì…˜)
                mask_save_path = None
                if save_foreground_masks:
                    mask_save_path = os.path.join(
                        mask_folder, 
                        f"mask_frame_{frame_idx}_obj_{det_idx}_{det.get('track_id', 'unknown')}.jpg"
                    )
                
                object_depth_info = extract_object_depth_info(
                    depth_array, det['box'],
                    use_foreground_only=use_foreground_only,
                    threshold_method=threshold_method,
                    save_mask=save_foreground_masks,
                    mask_save_path=mask_save_path
                )
            
            det_info = {
                "class_name": det.get('class_name', 'unknown'),
                "track_id": det.get('track_id', 'unknown'),
                "bounding_box": {
                    "x1": det['box'][0],
                    "y1": det['box'][1],
                    "x2": det['box'][2],
                    "y2": det['box'][3]
                }
            }
            
            # Foreground depth ì¶”ê°€ (ê°„ê²°)
            if object_depth_info and object_depth_info['depth_type'] == 'foreground_only':
                det_info["foreground_depth"] = {
                    "mean": object_depth_info['mean'],
                    "std": object_depth_info['std']
                }
            
            frame_metadata['detections'].append(det_info)
        
        # ê°œë³„ í”„ë ˆì„ JSON ì €ì¥ (explanations í¬í•¨)
        frame_json_path = os.path.join(per_frame_folder, f"frame_{frame_idx}_metadata.json")
        frame_with_explain = {
            "metadata": {
                "frame_info": frame_metadata,
                "histogram_method": histogram_method,
                "hist_weights": hist_weights,
                "post_filter": post_filter or {"applied": False}
            },
            "explanations": explanations
        }
        try:
            with open(frame_json_path, 'w', encoding='utf-8') as f:
                json.dump(frame_with_explain, f, indent=2, ensure_ascii=False)
            print(f"  âœ“ Saved: frame_{frame_idx}_metadata.json")
        except Exception as e:
            print(f"  âœ— Error saving frame {frame_idx}: {e}")
    
    # Summary JSON ìƒì„± ë° ì €ì¥ (ê°„ê²°)
    coverage_percentage = greedy_log['final_result'].get('final_coverage_percentage', '0%')
    summary = {
        "total_frames_selected": len(final_selected),
        "selected_frame_indices": [f['frame_idx'] for f in final_selected],
        "depth_enabled": depth_enabled,
        "threshold_method": threshold_method if depth_enabled else None,
        "use_foreground_only": use_foreground_only if depth_enabled else None,
        "histogram_method": histogram_method,
        "hist_weights": hist_weights,
        "post_filter": post_filter or {"applied": False},
        "coverage_statistics": {
            "final_covered_combos_count": greedy_log['final_result'].get('final_covered_combos_count', 0),
            "total_unique_combos_count": greedy_log['final_result'].get('total_unique_combos_count', 0),
            "coverage_percentage": coverage_percentage
        },
        "frame_details": [
            {
                "frame_index": f['frame_idx'],
                "selection_order": idx + 1,
                "new_combos_count": f.get('new_count', 0),
                "num_detections": len(f['detections'])
            }
            for idx, f in enumerate(final_selected)
        ],
        "explanations": explanations
    }
    
    summary_path = os.path.join(per_frame_folder, "summary.json")
    try:
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"\n  âœ“ Successfully saved summary.json")
        print(f"  âœ“ Total {len(final_selected)} per-frame JSON files saved in: {per_frame_folder}")
        if save_depth_visualization and dpt_model is not None:
            print(f"  âœ“ Depth visualizations saved in: {depth_vis_folder}")
        if save_foreground_masks and dpt_model is not None:
            print(f"  âœ“ Foreground masks saved in: {mask_folder}")
    except Exception as e:
        print(f"  âœ— Error saving summary: {e}")


def save_json_results(final_selected, greedy_log, output_folder, save_mode="both",
                      dpt_processor=None, dpt_model=None, dpt_device=None,
                      save_depth_visualization=True, save_foreground_masks=True,
                      use_foreground_only=True, threshold_method='otsu',
                      histogram_method="brightness_saturation_combined",
                      hist_weights={"brightness": 0.5, "saturation": 0.5},
                      post_filter=None):
    """
    [MODIFIED] JSON ì €ì¥ í†µí•© í•¨ìˆ˜ - ê¸°ì¡´ ëª¨ë“œì—ì„œ ê°„ê²° êµ¬ì¡°ë¡œ ë³€ê²½ (PLAN ê¸°ë°˜)
    """
    print(f"\n{'='*60}\nSaving JSON results (mode: {save_mode})\n{'='*60}")
    
    if save_mode in ["unified", "both"]:
        save_json_unified(
            final_selected, greedy_log, output_folder,
            dpt_processor, dpt_model, dpt_device,
            use_foreground_only, threshold_method,
            histogram_method, hist_weights, post_filter
        )
    
    if save_mode in ["per_frame", "both"]:
        save_json_per_frame(
            final_selected, greedy_log, output_folder,
            dpt_processor, dpt_model, dpt_device,
            save_depth_visualization, save_foreground_masks,
            use_foreground_only, threshold_method,
            histogram_method, hist_weights, post_filter
        )
    
    print(f"{'='*60}\n")


# ============================================================================
# PART 5: Main Pipeline (Post-Filter ì¶”ê°€ + ê°ì²´ í•„í„°ë§ ì¶”ê°€)
# ============================================================================

def main(video_path, output_folder, json_save_mode="both", model_type="yolo", model_path=None, torchreid_model_path=None,
         create_comparison_videos=True,  # ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì—¬ë¶€
         use_depth=True, dpt_model_name="Intel/dpt-large",
         save_depth_visualization=True, save_foreground_masks=True,
         use_foreground_only=True, threshold_method='otsu',
         hist_weight_brightness=0.5, hist_weight_saturation=0.5,
         apply_post_filter=True, post_hist_threshold=0.25, post_window_size=3, post_profile_iterations=1,
         frame_skip_interval=1,
         profile_iterations=1,
         show_available_classes=True,  # NEW: í´ë˜ìŠ¤ ëª©ë¡ í‘œì‹œ ì—¬ë¶€
         filter_mode=3,  # NEW: í•„í„°ë§ ëª¨ë“œ (1=ë¸”ë™ë¦¬ìŠ¤íŠ¸, 2=í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, 3=í•„í„°ë§ì—†ìŒ)
         filter_classes=None,  # NEW: í•„í„°ë§í•  í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì)
         tracker="botsort.yaml",  # NEW: Tracker configuration
         **kwargs):
    """
    [MODIFIED] ë©”ì¸ íŒŒì´í”„ë¼ì¸ - Depth ì¶”ì • + íˆìŠ¤í† ê·¸ë¨ ê²°í•© + Post-Filter ì¶”ê°€ + Frame Skipping + Profile Iterations + Window Size ì ìš© + ê°ì²´ í•„í„°ë§
    
    Args:
        ... (ê¸°ì¡´) + ìƒˆ íŒŒë¼ë¯¸í„°: 
        show_available_classes: True ì‹œ íƒì§€ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ëª©ë¡ ì¶œë ¥
        filter_mode: í•„í„°ë§ ëª¨ë“œ (1=ë¸”ë™ë¦¬ìŠ¤íŠ¸, 2=í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, 3=í•„í„°ë§ì—†ìŒ)
        filter_classes: í•„í„°ë§í•  í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì í˜¼ìš© ê°€ëŠ¥)
    """
    
    # ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  í•˜ìœ„ í´ë” ê²½ë¡œ ì¬êµ¬ì„±
    video_out_folder = os.path.join(output_folder, "0_source_videos")
    primary_select_folder = os.path.join(output_folder, "1_primary_selection_frames")
    profile_track_folder = os.path.join(output_folder, "2_profile_tracking_frames")
    after_reid_folder = os.path.join(output_folder, "3_after_reid_frames")
    final_folder = os.path.join(output_folder, "4_final_greedy_frames")

    for folder in [output_folder, primary_select_folder, 
                   profile_track_folder, after_reid_folder, final_folder]:
        if not os.path.exists(folder):
            os.makedirs(folder)
    
    if create_comparison_videos:
        if not os.path.exists(video_out_folder):
            os.makedirs(video_out_folder)
    
    # Load models (TorchReID + DPT)
    print("\n" + "="*80 + "\nSTEP 0: LOADING MODELS\n" + "="*80)
    detection_model, torchreid_model, device, class_names, dpt_processor, dpt_model, dpt_device = load_models(
        model_type=model_type,
        model_path=model_path,
        torchreid_model_path=torchreid_model_path,
        use_depth=use_depth, 
        dpt_model_name=dpt_model_name
    )
    
    # ğŸ†• í•„í„° ì„¤ì • ì²˜ë¦¬ (filter_modeì™€ filter_classesë¥¼ included_classes, excluded_classesë¡œ ë³€í™˜)
    print("\n" + "="*80 + "\nPROCESSING FILTER CONFIGURATION\n" + "="*80)
    
    # filter_classesê°€ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
    if filter_classes is None:
        filter_classes = []
    
    # í•„í„° ì„¤ì • ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
    included_classes, excluded_classes = process_filter_config(
        filter_mode=filter_mode,
        filter_classes=filter_classes,
        model=detection_model
    )
    
    # ğŸ†• í´ë˜ìŠ¤ ëª©ë¡ ì¶œë ¥ (ì„ íƒì‚¬í•­)
    if show_available_classes:
        print("\n" + "="*80 + "\nAVAILABLE CLASSES\n" + "="*80)
        print_available_classes(detection_model)
        print("="*80 + "\n")
    
    # STEP 0: ì¶”ì (Tracking) ë¹„êµ ì˜ìƒ ìƒì„± (ì˜µì…˜)
    if create_comparison_videos:
        print("\n" + "="*80 + "\nCREATING COMPARISON VIDEOS\n" + "="*80)
        no_tracking_video_path = os.path.join(video_out_folder, "video_no_tracking.mp4")
        create_video_from_source(video_path, detection_model, class_names, no_tracking_video_path,
                                use_tracking=False, included_classes=included_classes, excluded_classes=excluded_classes, tracker=tracker)

        tracking_video_path = os.path.join(video_out_folder, "video_with_tracking.mp4")
        create_video_from_source(video_path, detection_model, class_names, tracking_video_path,
                                use_tracking=True, included_classes=included_classes, excluded_classes=excluded_classes, tracker=tracker)

    # ë©”ì¸ íŒŒì´í”„ë¼ì¸
    print("\n" + "="*80 + "\nSTARTING MAIN KEYFRAME EXTRACTION PIPELINE\n" + "="*80)
    print(f"Frame skipping interval: {frame_skip_interval} (higher = faster processing, fewer frames)")
    print(f"Pre-Greedy Profile Tracking iterations: {profile_iterations}")
    print(f"Post-Greedy Profile Tracking iterations: {post_profile_iterations}")
    
    # Step 1: Primary Selection (with frame skipping + filtering)
    print("\nSTEP 1: Primary Selection...")
    selected_frames = primary_selection(video_path, detection_model, class_names,
                                       frame_skip_interval=frame_skip_interval,
                                       included_classes=included_classes,
                                       excluded_classes=excluded_classes,
                                       tracker=tracker)
    print(f"-> Found {len(selected_frames)} initial keyframes.")
    save_frames_as_images(selected_frames, primary_select_folder, prefix="primary_")
    
    # Step 2: Profile Tracking (ê²°í•© íˆìŠ¤í† ê·¸ë¨ + ë°˜ë³µ + Window Size ì ìš©)
    print("\nSTEP 2: Profile Tracking (Filtering)...")
    filtered_frames = profile_tracking(selected_frames, kwargs['window_size'], kwargs['hist_threshold'],
                                       hist_weight_brightness, hist_weight_saturation, profile_iterations)
    print(f"-> Filtered down to {len(filtered_frames)} frames after {profile_iterations} iterations.")
    save_frames_as_images(filtered_frames, profile_track_folder, prefix="profile_tracked_")
    
    # Step 3: Re-ID and Frame Merging (TorchReID ê¸°ë°˜ + filtering)
    print("\nSTEP 3: Unified TorchReID Re-ID and Frame Merging...")
    unique_frames = unified_torchreid_reid_and_frame_selection(
        filtered_frames, torchreid_model, device,
        reid_threshold=kwargs['reid_threshold'],
        frame_merge_threshold=kwargs['frame_merge_threshold'],
        min_samples=kwargs['min_reid_samples'],
        included_classes=included_classes,
        excluded_classes=excluded_classes
    )
    print(f"-> After Re-ID and merging, {len(unique_frames)} frames remain.")
    save_frames_as_images(unique_frames, after_reid_folder, prefix="after_reid_")

    # Step 4: Greedy coverage selection (with filtering)
    print("\nSTEP 4: Greedy Coverage Selection...")
    final_selected, greedy_log = greedy_coverage_selection(
        unique_frames,
        k=kwargs['coverage_k'],
        use_instance_id=kwargs['use_instance_id'],
        max_selected=kwargs['max_selected'],
        min_new_combos=kwargs['min_new_combos'],
        score_thresh=kwargs['score_thresh'],
        included_classes=included_classes,
        excluded_classes=excluded_classes
    )
    
    # Step 4.5: Post-Greedy Profile Tracking (ìƒˆ ì¶”ê°€, iterations ì§€ì›)
    post_filter_log = None
    if apply_post_filter:
        print("\nSTEP 4.5: Post-Greedy Profile Tracking...")
        final_selected, post_filter_log = post_greedy_profile_tracking(
            final_selected, post_hist_threshold,
            hist_weight_brightness, hist_weight_saturation, post_window_size, post_profile_iterations
        )
    
    # Step 5: Save final results and logs
    print("\n" + "="*80 + "\nSTEP 5: SAVING FINAL RESULTS AND LOGS\n" + "="*80)
    
    # ìµœì¢… ì´ë¯¸ì§€ ì €ì¥
    for item in final_selected:
        drawn_frame = draw_detections(item['frame'], item['detections'])
        img_path = os.path.join(final_folder, f"final_key_frame_{item['frame_idx']}.jpg")
        cv2.imwrite(img_path, drawn_frame)
        print(f"âœ“ Saved final keyframe: {os.path.basename(img_path)}")
    
    # JSON ì €ì¥ - ê°„ê²° êµ¬ì¡° + íˆìŠ¤í† ê·¸ë¨/í¬ìŠ¤íŠ¸ í•„í„° ì •ë³´ í¬í•¨
    save_json_results(
        final_selected, greedy_log, output_folder, 
        save_mode=json_save_mode,
        dpt_processor=dpt_processor,
        dpt_model=dpt_model,
        dpt_device=dpt_device,
        save_depth_visualization=save_depth_visualization,
        save_foreground_masks=save_foreground_masks,
        use_foreground_only=use_foreground_only,
        threshold_method=threshold_method,
        histogram_method="brightness_saturation_combined",
        hist_weights={"brightness": hist_weight_brightness, "saturation": hist_weight_saturation},
        post_filter=post_filter_log
    )
    
    print("\n" + "="*80 + "\nPIPELINE COMPLETE!\n" + "="*80)


# ============================================================================
# PART 6: Example Usage (config ì—…ë°ì´íŠ¸ - ê°ì²´ í•„í„°ë§ ì¶”ê°€)
# ============================================================================

if __name__ == "__main__":
    # í´ë˜ìŠ¤ ëª©ë¡ë§Œ ë³´ê¸° (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì•ˆ í•¨)
    if len(sys.argv) > 1 and sys.argv[1] == "--list-classes":
        list_classes_only(
            model_path="rtdetr-l.pt",  # configì˜ model_pathì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
            model_type="rt_detr"
        )
        sys.exit(0)
    
    config = {
        # ========================================
        # ê¸°ë³¸ ì„¤ì • (Basic Configuration)
        # ========================================
        "video_path": "KakaoTalk_20251003_113043691.mp4",  # ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ (e.g., MP4 íŒŒì¼)
        "output_folder": "results_output",  # ê²°ê³¼ íŒŒì¼(ì´ë¯¸ì§€, JSON, ë¹„ë””ì˜¤)ì„ ì €ì¥í•  í´ë” ê²½ë¡œ
        "json_save_mode": "both",  # JSON ì €ì¥ ëª¨ë“œ: "unified" (í•˜ë‚˜ì˜ ìš”ì•½ íŒŒì¼), "per_frame" (í”„ë ˆì„ë³„ íŒŒì¼), "both" (ë‘˜ ë‹¤)
        "create_comparison_videos": False,  # True ì‹œ 0_source_videos í´ë”ì— íŠ¸ë˜í‚¹ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± (False ì‹œ ìƒì„± ì•ˆ í•¨, ì‹œê°„ ì ˆì•½)
        
        # ========================================
        # ëª¨ë¸ ì„¤ì • (Model Configuration)
        # ========================================
        "model_type": "rt_detr",  # ê°ì§€ ëª¨ë¸ íƒ€ì…: "yolo" (YOLOv11) ë˜ëŠ” "rt_detr" (RT-DETR)
        #"model_path": "yolo11m.pt",  # YOLO/RT-DETR ëª¨ë¸ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ (.pt íŒŒì¼)
        "model_path": "rtdetr-l.pt",   # yolo ì‚¬ìš©ì‹œ 2ë²ˆì§¸ ì£¼ì„ í•´ì œí•˜ê³  3ë²ˆì§¸ ì£¼ì„(rt-detr ì‚¬ìš©ì‹œ ê·¸ë°˜ëŒ€)
        
        # ========================================
        # TorchReID ëª¨ë¸ ì„¤ì • (TorchReID Model Configuration)
        # ========================================
        "torchreid_model_path": "osnet_x1_0_market_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth",  # TorchReID OSNet ëª¨ë¸ ê²½ë¡œ (.pth íŒŒì¼). None ì‹œ pretrained weights ìë™ ë‹¤ìš´ë¡œë“œ
        
        # ========================================
        # Depth ì¶”ì • ì„¤ì • (Depth Estimation Configuration)
        # ========================================
        "use_depth": True,  # True ì‹œ DPT ëª¨ë¸ë¡œ Depth ì¶”ì • í™œì„±í™” (False ì‹œ ë¹„í™œì„±í™”, ì†ë„ í–¥ìƒ)
        "dpt_model_name": "Intel/dpt-hybrid-midas",  # DPT ëª¨ë¸ ì´ë¦„: "Intel/dpt-large" (ê³ í’ˆì§ˆ/ëŠë¦¼), "Intel/dpt-hybrid-midas" (ì¤‘ê°„), "Intel/dpt-swinv2-tiny-256" (ë¹ ë¦„/ì €í’ˆì§ˆ)
        "save_depth_visualization": True,  # True ì‹œ Depth ë§µì„ ì»¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ì €ì¥ (depth_visualizations í´ë”)
        "save_foreground_masks": True,  # True ì‹œ ê°ì²´ë³„ ì „ê²½ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ ì €ì¥ (foreground_masks í´ë”)
        "use_foreground_only": True,  # True ì‹œ Depth í†µê³„ì—ì„œ ì „ê²½(ê°ì²´) ì˜ì—­ë§Œ ì‚¬ìš© (False ì‹œ ì „ì²´ ì˜ì—­)
        "threshold_method": "otsu",  # ì „ê²½/ë°°ê²½ ë¶„ë¦¬ ë°©ë²•: "otsu" (Otsu ìë™ ì„ê³„ê°’), "mean" (í‰ê· ê°’ ê¸°ì¤€)
        
        # ========================================
        # Frame Skipping ì„¤ì • (Frame Skipping Configuration)
        # ========================================
        "frame_skip_interval": 5,  # ë¹„ë””ì˜¤ í”„ë ˆì„ ìŠ¤í‚µ ê°„ê²© (1 = ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬, 5 = 5í”„ë ˆì„ë§ˆë‹¤ í•˜ë‚˜ ì²˜ë¦¬, ë†’ì„ìˆ˜ë¡ ë¹ ë¦„)
        
        # ========================================
        # Profile Tracking ë°˜ë³µ ì„¤ì • (Profile Tracking Iterations Configuration)
        # ========================================
        "profile_iterations": 3,  # Pre-Greedy Profile Tracking ë°˜ë³µ íšŸìˆ˜ (1 = ê¸°ë³¸ í•œ ë²ˆ, 2+ = ë°˜ë³µ í•„í„°ë§ìœ¼ë¡œ ë” ì—„ê²©í•œ ì œê±°)
        
        # ========================================
        # Post-Greedy Profile Tracking ì„¤ì • (Post-Greedy Profile Tracking Configuration)
        # ========================================
        "apply_post_filter": True,  # True ì‹œ Greedy í›„ ì¶”ê°€ ìœ ì‚¬ í”„ë ˆì„ í•„í„°ë§ ì ìš© (False ì‹œ ìƒëµ)
        "post_hist_threshold": 0.25,  # Post-Filter íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ì—„ê²©)
        "post_window_size": 7,  # Post-Filter í•„í„°ë§ ì‹œ ê³ ë ¤í•  ì£¼ë³€ í”„ë ˆì„ ìˆ˜ (e.g., 3 = Â±1 í”„ë ˆì„)
        "post_profile_iterations": 1,  # Post-Greedy Profile Tracking ë°˜ë³µ íšŸìˆ˜ (1 = ê¸°ë³¸ í•œ ë²ˆ, 2+ = ë°˜ë³µ í•„í„°ë§ìœ¼ë¡œ ë” ì—„ê²©í•œ ì œê±°)
        
        # ========================================
        # íˆìŠ¤í† ê·¸ë¨ ì„¤ì • (Histogram Configuration for Profile Tracking)
        # ========================================
        "hist_weight_brightness": 0.5,  # ë°ê¸°(Grayscale) íˆìŠ¤í† ê·¸ë¨ ê°€ì¤‘ì¹˜ (brightness + saturation = 1.0ì´ ë˜ë„ë¡ ì„¤ì •)
        "hist_weight_saturation": 0.5,  # ì±„ë„(HSV S ì±„ë„) íˆìŠ¤í† ê·¸ë¨ ê°€ì¤‘ì¹˜ (brightness + saturation = 1.0ì´ ë˜ë„ë¡ ì„¤ì •)
        
        # ========================================
        # Profile Tracking (Pre-Greedy) ì„¤ì • (Pre-Greedy Profile Tracking Configuration)
        # ========================================
        "window_size": 15,  # Pre-Greedy í•„í„°ë§ ì‹œ ê³ ë ¤í•  ì£¼ë³€ í”„ë ˆì„ ìˆ˜ (e.g., 5 = Â±2 í”„ë ˆì„)
        "hist_threshold": 0.3,  # Pre-Greedy íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ í”„ë ˆì„ ì œê±° ì—„ê²©)
        
        # ========================================
        # TorchReID ì„¤ì • (TorchReID Configuration for Re-ID)
        # ========================================
        "reid_threshold": 0.8,  # TorchReID ìœ ì‚¬ë„ ì„ê³„ê°’ (ë†’ì„ìˆ˜ë¡ ì—„ê²©)
        
        # ========================================
        # Re-ID ë° í”„ë ˆì„ ë³‘í•© ì„¤ì • (Re-ID and Frame Merging Configuration)
        # ========================================
        "min_reid_samples": 2,  # Re-ID í´ëŸ¬ìŠ¤í„°ë§ ìµœì†Œ ê°ì²´ ìƒ˜í”Œ ìˆ˜ (í´ë˜ìŠ¤ë‹¹)
        "frame_merge_threshold": 0.7,  # í”„ë ˆì„ ë³‘í•© ì‹œ TorchReID ìœ ì‚¬ë„ ì„ê³„ê°’ (ë†’ì„ìˆ˜ë¡ ì—„ê²©)
        
        # ========================================
        # Greedy Selection ì„¤ì • (Greedy Coverage Selection Configuration)
        # ========================================
        "coverage_k": 2,  # Greedyì—ì„œ ì»¤ë²„í•  ê°ì²´ ì¡°í•© í¬ê¸° (e.g., 2 = ê°ì²´ ìŒ)
        "use_instance_id": True,  # True ì‹œ track_id(ì¸ìŠ¤í„´ìŠ¤ ID) ì‚¬ìš©, False ì‹œ class_nameë§Œ ì‚¬ìš©
        "max_selected": None,  # ìµœëŒ€ ì„ íƒí•  í‚¤í”„ë ˆì„ ìˆ˜ (None = ë¬´ì œí•œ, int = ì œí•œ)
        "min_new_combos": 1,  # ì„ íƒí•  í”„ë ˆì„ì´ ì¶”ê°€ë¡œ ì»¤ë²„í•  ìµœì†Œ ì‹ ê·œ ì¡°í•© ìˆ˜ (0 ì´í•˜ ì‹œ ëª¨ë“  í”„ë ˆì„ ì„ íƒ)
        "score_thresh": 0.0,  # ê°ì²´ ê°ì§€ ì‹ ë¢°ë„ ì„ê³„ê°’ (0.0 ~ 1.0, ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ ê°ì²´ í¬í•¨)
        
        # ========================================
        # ğŸ†• ê°ì²´ í•„í„°ë§ ì„¤ì • (Object Filtering Configuration)
        # ========================================
        "show_available_classes": True,  # True ì‹œ íƒì§€ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ëª©ë¡ ì¶œë ¥ (ì²˜ìŒ ì‹¤í–‰ ì‹œ True ê¶Œì¥)
        
        # í•„í„°ë§ ëª¨ë“œ ì„¤ì • (ì•„ë˜ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ ì„ íƒ)
        # 1 = ë¸”ë™ë¦¬ìŠ¤íŠ¸ (íŠ¹ì • í´ë˜ìŠ¤ ì œì™¸, ë‚˜ë¨¸ì§€ ëª¨ë‘ í¬í•¨)
        # 2 = í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (íŠ¹ì • í´ë˜ìŠ¤ë§Œ í¬í•¨, ë‚˜ë¨¸ì§€ ëª¨ë‘ ì œì™¸)
        # 3 = í•„í„°ë§ ì—†ìŒ (ëª¨ë“  í´ë˜ìŠ¤ í¬í•¨)
        "filter_mode": 3,
        
        # í•„í„°ë§í•  í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (filter_modeê°€ 1 ë˜ëŠ” 2ì¼ ë•Œë§Œ ì‚¬ìš©)
        # - ë¬¸ìì—´ ë˜ëŠ” ìˆ«ì(í´ë˜ìŠ¤ ID)ë¡œ ì§€ì • ê°€ëŠ¥
        # - ì˜ˆì‹œ: ["person", "car", 2, 7]  # "person", "car", í´ë˜ìŠ¤ID 2(car), í´ë˜ìŠ¤ID 7(truck)
        "filter_classes": [],
        
        # ========================================
        # ì‚¬ìš© ì˜ˆì‹œ:
        # ========================================
        # ì˜ˆì‹œ 1: ë¸”ë™ë¦¬ìŠ¤íŠ¸ - personê³¼ car ì œì™¸
        # "filter_mode": 1,
        # "filter_classes": ["person", "car"],
        
        # ì˜ˆì‹œ 2: ë¸”ë™ë¦¬ìŠ¤íŠ¸ - í´ë˜ìŠ¤ IDë¡œ ì§€ì • (person=0, car=2)
        # "filter_mode": 1,
        # "filter_classes": [0, 2],
        
        # ì˜ˆì‹œ 3: ë¸”ë™ë¦¬ìŠ¤íŠ¸ - í˜¼í•© ì‚¬ìš©
        # "filter_mode": 1,
        # "filter_classes": ["person", 2, "truck"],  # person, car(2), truck
        
        # ì˜ˆì‹œ 4: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ - cell phoneê³¼ bottleë§Œ í¬í•¨
        # "filter_mode": 2,
        # "filter_classes": ["cell phone", "bottle"],
        
        # ì˜ˆì‹œ 5: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ - í´ë˜ìŠ¤ IDë¡œ ì§€ì •
        # "filter_mode": 2,
        # "filter_classes": [67, 39],  # cell phone(67), bottle(39)
        
        # ì˜ˆì‹œ 6: í•„í„°ë§ ì—†ìŒ (í˜„ì¬ ì„¤ì •)
        # "filter_mode": 3,
        # "filter_classes": [],  # filter_mode=3ì¼ ë•ŒëŠ” ë¬´ì‹œë¨
        
        # âš ï¸ ì£¼ì˜ì‚¬í•­:
        # - filter_mode=3ì¼ ë•Œ filter_classesì— ê°’ì´ ìˆìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥ í›„ ë¬´ì‹œë©ë‹ˆë‹¤
        # - filter_mode=1 ë˜ëŠ” 2ì¼ ë•Œ filter_classesê°€ ë¹„ì–´ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ mode=3ìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤
        # - í´ë˜ìŠ¤ IDëŠ” --list-classes ì˜µì…˜ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        
        # ========================================
    }
    
    main(**config)
